{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "# Load the graph and data\n",
    "G = nx.read_gml(\"GraphMissingEdges.gml\")\n",
    "df_edges = pd.read_csv(\"edgesToEvaluate.csv\")\n",
    "df_categories = pd.read_csv(\"categories.csv\")\n",
    "\n",
    "# Create a category mapping from the CSV file\n",
    "category_map = dict(zip(df_categories[\"CategoryId\"], df_categories[\"names\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Feature Engineering\n",
    "Weâ€™ll generate several features, including:\n",
    "\n",
    "- Common categories between two venues.\n",
    "- Jaccard similarity and Adamic-Adar index for node similarity.\n",
    "- Distance between the venues based on latitude and longitude.\n",
    "- Stars difference and review count difference between the venues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.algorithms import link_prediction\n",
    "from geopy.distance import geodesic\n",
    "import numpy as np\n",
    "\n",
    "def calculate_features(row):\n",
    "    venue1, venue2 = row['venue1'], row['venue2']\n",
    "    \n",
    "    # Categories-based feature\n",
    "    cat1 = set(G.nodes[venue1][\"categories\"].split(\",\"))\n",
    "    cat2 = set(G.nodes[venue2][\"categories\"].split(\",\"))\n",
    "    common_categories = len(cat1.intersection(cat2))\n",
    "    \n",
    "    # Node similarity metrics\n",
    "    jaccard = list(link_prediction.jaccard_coefficient(G, [(venue1, venue2)]))[0][2]\n",
    "\n",
    "    # Handle nodes with zero-degree by filtering in adamic_adar calculation\n",
    "    adamic_adar = 0\n",
    "    for u, v, adamic_adar_score in link_prediction.adamic_adar_index(G, [(venue1, venue2)]):\n",
    "        # Filter nodes with non-zero degree\n",
    "        valid_neighbors = [\n",
    "            w for w in nx.common_neighbors(G, u, v)\n",
    "            if G.degree(w) > 0\n",
    "        ]\n",
    "        # Calculate adamic_adar_score only for valid neighbors\n",
    "        adamic_adar = sum(1 / np.log(G.degree(w)) for w in valid_neighbors)\n",
    "    \n",
    "    # Distance-based feature\n",
    "    loc1 = (G.nodes[venue1][\"latitude\"], G.nodes[venue1][\"longitude\"])\n",
    "    loc2 = (G.nodes[venue2][\"latitude\"], G.nodes[venue2][\"longitude\"])\n",
    "    distance = geodesic(loc1, loc2).kilometers\n",
    "    \n",
    "    # Difference in stars and review count\n",
    "    # Convert stars and review count to numeric values\n",
    "    stars1 = float(G.nodes[venue1][\"stars\"])\n",
    "    stars2 = float(G.nodes[venue2][\"stars\"])\n",
    "    review_count1 = int(G.nodes[venue1][\"reviewCount\"])\n",
    "    review_count2 = int(G.nodes[venue2][\"reviewCount\"])\n",
    "    \n",
    "    stars_diff = abs(stars1 - stars2)\n",
    "    review_count_diff = abs(review_count1 - review_count2)\n",
    "    \n",
    "    return pd.Series({\n",
    "        \"common_categories\": common_categories,\n",
    "        \"jaccard\": jaccard,\n",
    "        # \"adamic_adar\": adamic_adar,\n",
    "        \"distance\": distance,\n",
    "        \"stars_diff\": stars_diff,\n",
    "        \"review_count_diff\": review_count_diff\n",
    "    })\n",
    "\n",
    "# Apply feature calculation to each row in edges dataset\n",
    "df_features = df_edges.apply(calculate_features, axis=1)\n",
    "df_edges = pd.concat([df_edges, df_features], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Prepare Labels\n",
    "Load the ground truth labels from edgesToEvaluate.csv. Make sure to have a column named link that indicates the existence of the link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m df_train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df_positive, df_negative], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Use the calculate_features function defined earlier to create features for the training set\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m df_train_features \u001b[38;5;241m=\u001b[39m \u001b[43mdf_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcalculate_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m df_train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df_train, df_train_features], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Define features and labels\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py:9568\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   9557\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m   9559\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m   9560\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   9561\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9566\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m   9567\u001b[0m )\n\u001b[0;32m-> 9568\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py:764\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw()\n\u001b[0;32m--> 764\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py:891\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 891\u001b[0m     results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    893\u001b[0m     \u001b[38;5;66;03m# wrap results\u001b[39;00m\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py:907\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    905\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m    906\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m--> 907\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m    909\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m    910\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m    911\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[17], line 18\u001b[0m, in \u001b[0;36mcalculate_features\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Handle nodes with zero-degree by filtering in adamic_adar calculation\u001b[39;00m\n\u001b[1;32m     17\u001b[0m adamic_adar \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m u, v, adamic_adar_score \u001b[38;5;129;01min\u001b[39;00m link_prediction\u001b[38;5;241m.\u001b[39madamic_adar_index(G, [(venue1, venue2)]):\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# Filter nodes with non-zero degree\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     valid_neighbors \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     21\u001b[0m         w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m nx\u001b[38;5;241m.\u001b[39mcommon_neighbors(G, u, v)\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m G\u001b[38;5;241m.\u001b[39mdegree(w) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     23\u001b[0m     ]\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# Calculate adamic_adar_score only for valid neighbors\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/networkx/algorithms/link_prediction.py:45\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m G:\n\u001b[1;32m     44\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m nx\u001b[38;5;241m.\u001b[39mNodeNotFound(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in G.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ((u, v, \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m u, v \u001b[38;5;129;01min\u001b[39;00m ebunch)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/networkx/algorithms/link_prediction.py:235\u001b[0m, in \u001b[0;36madamic_adar_index.<locals>.predict\u001b[0;34m(u, v)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(u, v):\n\u001b[0;32m--> 235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdegree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommon_neighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/networkx/algorithms/link_prediction.py:235\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(u, v):\n\u001b[0;32m--> 235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdegree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m nx\u001b[38;5;241m.\u001b[39mcommon_neighbors(G, u, v))\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Positive samples (existing edges)\n",
    "positive_edges = list(G.edges)\n",
    "df_positive = pd.DataFrame(positive_edges, columns=[\"venue1\", \"venue2\"])\n",
    "df_positive['link'] = 1  # Label as positive\n",
    "\n",
    "# Negative samples (non-existent edges)\n",
    "nodes = list(G.nodes)\n",
    "negative_edges = []\n",
    "while len(negative_edges) < len(positive_edges):  # Match number of positive examples\n",
    "    u, v = random.sample(nodes, 2)\n",
    "    if not G.has_edge(u, v):\n",
    "        negative_edges.append((u, v))\n",
    "\n",
    "df_negative = pd.DataFrame(negative_edges, columns=[\"venue1\", \"venue2\"])\n",
    "df_negative['link'] = 0  # Label as negative\n",
    "\n",
    "# Combine positive and negative samples\n",
    "df_train = pd.concat([df_positive, df_negative], ignore_index=True)\n",
    "\n",
    "# Use the calculate_features function defined earlier to create features for the training set\n",
    "df_train_features = df_train.apply(calculate_features, axis=1)\n",
    "df_train = pd.concat([df_train, df_train_features], axis=1)\n",
    "\n",
    "# Define features and labels\n",
    "X_train = df_train.drop(columns=['venue1', 'venue2', 'link'])\n",
    "y_train = df_train['link']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Train the Model\n",
    "Weâ€™ll use a classifier like Random Forest for simplicity. You can also try other classifiers such as logistic regression or XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Save Predictions\n",
    "Finally, save the predictions on edgesToEvaluate.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edges['link'] = clf.predict(X)\n",
    "df_edges[['linkID', 'link']].to_csv(\"predicted_links.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
